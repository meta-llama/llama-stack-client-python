[project]
name = "llama_stack_client"
version = "0.2.9"
description = "The official Python library for the llama-stack-client API"
dynamic = ["readme"]
license = "Apache-2.0"
authors = [{ name = "Meta Llama", email = "llama-oss@meta.com" }]
dependencies = [
    "httpx>=0.23.0, <1",
    "pydantic>=1.9.0, <3",
    "typing-extensions>=4.7, <5",
    "anyio>=3.5.0, <5",
    "distro>=1.7.0, <2",
    "sniffio",
    "cached-property; python_version < '3.8'",
    "tqdm",
    "rich",
    "click",
    "pyaml",
    "prompt_toolkit",
    "pandas",
    "termcolor",
]
requires-python = ">= 3.7"
classifiers = [
  "Typing :: Typed",
  "Intended Audience :: Developers",
  "Programming Language :: Python :: 3.7",
  "Programming Language :: Python :: 3.8",
  "Programming Language :: Python :: 3.9",
  "Programming Language :: Python :: 3.10",
  "Programming Language :: Python :: 3.11",
  "Programming Language :: Python :: 3.12",
  "Operating System :: OS Independent",
  "Operating System :: POSIX",
  "Operating System :: MacOS",
  "Operating System :: POSIX :: Linux",
  "Operating System :: Microsoft :: Windows",
  "Topic :: Software Development :: Libraries :: Python Modules",
  "License :: OSI Approved :: Apache Software License"
]

[project.urls]
Homepage = "https://github.com/meta-llama/llama-stack-client-python"
Repository = "https://github.com/meta-llama/llama-stack-client-python"



[build-system]
requires = ["hatchling", "hatch-fancy-pypi-readme"]
build-backend = "hatchling.build"

[tool.hatch.build]
include = [
  "src/*"
]

[tool.hatch.build.targets.wheel]
packages = ["src/llama_stack_client"]

[tool.hatch.build.targets.sdist]
# Basically everything except hidden files/directories (such as .github, .devcontainers, .python-version, etc)
include = [
  "/*.toml",
  "/*.json",
  "/*.lock",
  "/*.md",
  "/mypy.ini",
  "/noxfile.py",
  "bin/*",
  "examples/*",
  "src/*",
  "tests/*",
]

[tool.hatch.metadata.hooks.fancy-pypi-readme]
content-type = "text/markdown"

[[tool.hatch.metadata.hooks.fancy-pypi-readme.fragments]]
path = "README.md"

[[tool.hatch.metadata.hooks.fancy-pypi-readme.substitutions]]
# replace relative links with absolute links
pattern = '\[(.+?)\]\(((?!https?://)\S+?)\)'
replacement = '[\1](https://github.com/meta-llama/llama-stack-client-python/tree/main/\g<2>)'

[tool.ruff]
line-length = 120
output-format = "grouped"

[tool.ruff.format]
docstring-code-format = true

[tool.ruff.lint]
select = [
  # isort
  "I",
  # bugbear rules
  "B",
  # remove unused imports
  "F401",
  # bare except statements
  "E722",
  # unused arguments
  "ARG",
  # print statements
  "T201",
  "T203",
  # misuse of typing.TYPE_CHECKING
  "TCH004",
  # import rules
  "TID251",
]
ignore = [
  # mutable defaults
  "B006",
]
unfixable = [
  # disable auto fix for print statements
  "T201",
  "T203",
]

[tool.ruff.lint.flake8-tidy-imports.banned-api]
"functools.lru_cache".msg = "This function does not retain type information for the wrapped function's arguments; The `lru_cache` function from `_utils` should be used instead"

[tool.ruff.lint.isort]
length-sort = true
length-sort-straight = true
combine-as-imports = true
extra-standard-library = ["typing_extensions"]
known-first-party = ["llama_stack_client", "tests"]

[tool.ruff.lint.per-file-ignores]
"bin/**.py" = ["T201", "T203"]
"scripts/**.py" = ["T201", "T203"]
"tests/**.py" = ["T201", "T203"]
"examples/**.py" = ["T201", "T203", "TCH004", "I", "B"]
"src/llama_stack_client/lib/**.py" = ["T201", "T203", "TCH004", "I", "B"]

[project.scripts]
llama-stack-client = "llama_stack_client.lib.cli.llama_stack_client:main"
